{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62204bfc",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Programming - CSCN 8020\n",
    "\n",
    "## Assignment 1\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "### Done by ***Eris Leksi***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3c5c27",
   "metadata": {},
   "source": [
    "Problem 2 [20]\n",
    "\n",
    "\n",
    "Problem Statement\n",
    "\n",
    "2x2 Gridworld: Consider a 2x2 gridworld with the following characteristics:\n",
    "\n",
    "• State Space (S): s1, s2, s3, s4.\n",
    "\n",
    "• Action Space (A): up, down, left, right.\n",
    "\n",
    "• Initial Policy (π): For all states, π(up|s) = 1.\n",
    "\n",
    "• Transition Probabilities P(s′|s, a):\n",
    "\n",
    "– If the action is valid (does not run into a wall), the transition is deterministic.\n",
    "– Otherwise, s′ = s.\n",
    "\n",
    "• Rewards R(s):\n",
    "\n",
    "– R(s1) = 5 for all actions a.\n",
    "\n",
    "\n",
    "Figure 1: 2x2 Gridworld\n",
    "\n",
    "\n",
    "1– R(s2) = 10 for all actions a.\n",
    "\n",
    "– R(s3) = 1 for all actions a.\n",
    "\n",
    "– R(s4) = 2 for all actions a.\n",
    "\n",
    "\n",
    "Tasks\n",
    "\n",
    "Perform two iterations of Value Iteration for this gridworld environment. Show the step-by-step process\n",
    "(without code) including policy evaluation and policy improvement. Provide the following for each\n",
    "iteration:\n",
    "\n",
    "• Iteration 1:\n",
    "\n",
    "1. Show the initial value function (V) for each state.\n",
    "\n",
    "2. Perform value function updates.\n",
    "\n",
    "3. Show the updated value function.\n",
    "\n",
    "• Iteration 2: Show the value function (V) after the second iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4783e1c",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53abdd73",
   "metadata": {},
   "source": [
    "**States**: {s1, s2, s3, s4}  \n",
    "- **Actions**: {up, down, left, right}  \n",
    "- **Transitions**: deterministic. If an action would leave the grid, the agent stays in place.  \n",
    "- **Rewards** (state-based, action-independent):  \n",
    "  - R(s1) = 5  \n",
    "  - R(s2) = 10  \n",
    "  - R(s3) = 1  \n",
    "  - R(s4) = 2  \n",
    "- **Discount factor**: γ = 0.9  \n",
    "The update rule for value iteration is:\n",
    "\n",
    "$$\n",
    "V(k+1)(s) = R(s) + γ * max[a ∈ A] V(k)( T(s,a) )\n",
    "$$ \n",
    "\n",
    "### Explanation of Terms\n",
    "- **V(k+1)(s):** Updated value of state *s* at iteration (k+1)  \n",
    "- **R(s):** Immediate reward for being in state *s*  \n",
    "- **γ (gamma):** Discount factor, with 0 ≤ γ < 1  \n",
    "- **A:** Set of all possible actions  \n",
    "- **T(s,a):** The next state reached from state *s* after taking action *a*  \n",
    "- **\\(T(s,a)\\):** The next state reached from state \\(s\\) after taking action \\(a\\)  \n",
    "\n",
    "\n",
    "Let's also consider the Grid Layout as follows:\n",
    "\n",
    "s1   s2\n",
    "\n",
    "s3   s4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cfc737",
   "metadata": {},
   "source": [
    "# Initialization (Iteration 0)\n",
    "\n",
    "Start with all values at zero:\n",
    "\n",
    "- $V_0(s_1) = 0$  \n",
    "- $V_0(s_2) = 0$  \n",
    "- $V_0(s_3) = 0$  \n",
    "- $V_0(s_4) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffbe312",
   "metadata": {},
   "source": [
    "# Iteration 1 – Value Iteration\n",
    "\n",
    "We begin with the **initial value function** \\( V_0(s) \\), where all state values are initialized to zero:\n",
    "\n",
    "| State   | \\( V_0(s) \\) |\n",
    "|---------|--------------|\n",
    "| \\( s_1 \\) | 0 |\n",
    "| \\( s_2 \\) | 0 |\n",
    "| \\( s_3 \\) | 0 |\n",
    "| \\( s_4 \\) | 0 |\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Apply Bellman Backup\n",
    "\n",
    "The Bellman update equation is:\n",
    "\n",
    "\\[\n",
    "V_{k+1}(s) = \\max_a \\Big[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V_k(s') \\Big]\n",
    "\\]\n",
    "\n",
    "Since rewards are **state-based and action-independent**, and \\( V_0(s) = 0 \\), the update simplifies to:\n",
    "\n",
    "\\[\n",
    "V_1(s) = R(s)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Compute New Values\n",
    "\n",
    "- **State \\( s_1 \\):**  \n",
    "\n",
    "  $$\n",
    "  V_1(s_1) = 5\n",
    "  $$\n",
    "\n",
    "- **State \\( s_2 \\):**  \n",
    "  $$\n",
    "  V_1(s_2) = 10\n",
    "  $$\n",
    "\n",
    "- **State \\( s_3 \\):**  \n",
    "  $$\n",
    "  V_1(s_3) = 1\n",
    "  $$\n",
    "\n",
    "- **State \\( s_4 \\):**  \n",
    "  $$\n",
    "  V_1(s_4) = 2\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Updated Value Function\n",
    "\n",
    "| State   | \\( V_1(s) \\) |\n",
    "|---------|--------------|\n",
    "| \\( s_1 \\) | 5 |\n",
    "| \\( s_2 \\) | 10 |\n",
    "| \\( s_3 \\) | 1 |\n",
    "| \\( s_4 \\) | 2 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58fd1ff",
   "metadata": {},
   "source": [
    "# Iteration 2 – Value Update Calculations (All Actions Corrected)\n",
    "\n",
    "We use the **Bellman update equation**:\n",
    "\n",
    "$$\n",
    "V_{k+1}(s) = \\max_a \\sum_{s'} P(s'|s,a) \\big[ R(s,a,s') + \\gamma V_k(s') \\big]\n",
    "$$\n",
    "\n",
    "Assume discount factor $\\gamma = 0.9$.  \n",
    "\n",
    "For each state, we consider **all four actions**: up, down, left, right. If an action goes off the grid, it remains in the same state.\n",
    "\n",
    "---\n",
    "\n",
    "## State s1\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Action up: } & R(s_1) + \\gamma V_1(s_1) = 5 + 0.9 \\cdot 5 = 5 + 4.5 = 9.5 \\\\\n",
    "\\text{Action down: } & R(s_1) + \\gamma V_1(s_3) = 5 + 0.9 \\cdot 1 = 5 + 0.9 = 5.9 \\\\\n",
    "\\text{Action left: } & R(s_1) + \\gamma V_1(s_1) = 5 + 4.5 = 9.5 \\\\\n",
    "\\text{Action right: } & R(s_1) + \\gamma V_1(s_2) = 5 + 0.9 \\cdot 10 = 5 + 9 = 14\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V_2(s_1) = \\max(9.5, 5.9, 9.5, 14) = 14\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## State s2\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Action up: } & R(s_2) + \\gamma V_1(s_2) = 10 + 0.9 \\cdot 10 = 10 + 9 = 19 \\\\\n",
    "\\text{Action down: } & R(s_2) + \\gamma V_1(s_4) = 10 + 0.9 \\cdot 2 = 10 + 1.8 = 11.8 \\\\\n",
    "\\text{Action left: } & R(s_2) + \\gamma V_1(s_1) = 10 + 0.9 \\cdot 5 = 10 + 4.5 = 14.5 \\\\\n",
    "\\text{Action right: } & R(s_2) + \\gamma V_1(s_2) = 10 + 9 = 19\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V_2(s_2) = \\max(19, 11.8, 14.5, 19) = 19\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## State s3\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Action up: } & R(s_3) + \\gamma V_1(s_1) = 1 + 0.9 \\cdot 5 = 1 + 4.5 = 5.5 \\\\\n",
    "\\text{Action down: } & R(s_3) + \\gamma V_1(s_3) = 1 + 0.9 \\cdot 1 = 1 + 0.9 = 1.9 \\\\\n",
    "\\text{Action left: } & R(s_3) + \\gamma V_1(s_3) = 1 + 0.9 = 1.9 \\\\\n",
    "\\text{Action right: } & R(s_3) + \\gamma V_1(s_4) = 1 + 0.9 \\cdot 2 = 1 + 1.8 = 2.8\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V_2(s_3) = \\max(5.5, 1.9, 1.9, 2.8) = 5.5\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## State s4\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Action up: } & R(s_4) + \\gamma V_1(s_2) = 2 + 0.9 \\cdot 10 = 2 + 9 = 11 \\\\\n",
    "\\text{Action down: } & R(s_4) + \\gamma V_1(s_4) = 2 + 0.9 \\cdot 2 = 2 + 1.8 = 3.8 \\\\\n",
    "\\text{Action left: } & R(s_4) + \\gamma V_1(s_3) = 2 + 0.9 \\cdot 1 = 2 + 0.9 = 2.9 \\\\\n",
    "\\text{Action right: } & R(s_4) + \\gamma V_1(s_4) = 2 + 1.8 = 3.8\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V_2(s_4) = \\max(11, 3.8, 2.9, 3.8) = 11\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Summary after Iteration 2\n",
    "\n",
    "$$\n",
    "V_2 = \\{ s_1: 14, \\ s_2: 19, \\ s_3: 5.5, \\ s_4: 11 \\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57435f7a",
   "metadata": {},
   "source": [
    "As expected, the values increased once the agent could look “one step further” into the future. States that connect quickly to the high-reward state s2 (like s1 and s4) gained the most. By the second iteration, we already see the greedy policy forming:  \n",
    "\n",
    "- s1 → go right to s2  \n",
    "- s2 → stay in s2 (up/right loop)  \n",
    "- s3 → go up to s1  \n",
    "- s4 → go up to s2  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec4f60",
   "metadata": {},
   "source": [
    "# Value Iteration Table\n",
    "\n",
    "| State | V₀ (Iteration 0) | V₁ (Iteration 1) | V₂ (Iteration 2) |\n",
    "|-------|-----------------|-----------------|-----------------|\n",
    "| s1    | 0               | 5               | 14              |\n",
    "| s2    | 0               | 10              | 19              |\n",
    "| s3    | 0               | 1               | 5.5             |\n",
    "| s4    | 0               | 2               | 11              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac61a7c8",
   "metadata": {},
   "source": [
    "### Extra code for verification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7651143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: {'s1': 0.0, 's2': 0.0, 's3': 0.0, 's4': 0.0}\n",
      "Iteration 1: {'s1': 5.0, 's2': 10.0, 's3': 1.0, 's4': 2.0}  argmax: {'s1': ('up', 's1'), 's2': ('up', 's2'), 's3': ('up', 's1'), 's4': ('up', 's2')}\n",
      "Iteration 2: {'s1': 14.0, 's2': 19.0, 's3': 5.5, 's4': 11.0}  argmax: {'s1': ('right', 's2'), 's2': ('up', 's2'), 's3': ('up', 's1'), 's4': ('up', 's2')}\n"
     ]
    }
   ],
   "source": [
    "# Discount factor (how much we value future rewards compared to immediate ones)\n",
    "gamma = 0.9  \n",
    "\n",
    "# Define the set of states in the 2x2 grid\n",
    "states = [\"s1\", \"s2\", \"s3\", \"s4\"]\n",
    "\n",
    "# Rewards for each state (state-based rewards, independent of action)\n",
    "R = {\n",
    "    \"s1\": 5,\n",
    "    \"s2\": 10,\n",
    "    \"s3\": 1,\n",
    "    \"s4\": 2\n",
    "}\n",
    "\n",
    "# Possible actions the agent can take\n",
    "A = [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "# Transition function T[s][a] → next state\n",
    "# If the action leads outside the grid, the agent stays in the same state\n",
    "T = {\n",
    "    \"s1\": {\"up\":\"s1\",\"down\":\"s3\",\"left\":\"s1\",\"right\":\"s2\"},\n",
    "    \"s2\": {\"up\":\"s2\",\"down\":\"s4\",\"left\":\"s1\",\"right\":\"s2\"},\n",
    "    \"s3\": {\"up\":\"s1\",\"down\":\"s3\",\"left\":\"s3\",\"right\":\"s4\"},\n",
    "    \"s4\": {\"up\":\"s2\",\"down\":\"s4\",\"left\":\"s3\",\"right\":\"s4\"},\n",
    "}\n",
    "\n",
    "# One step of Value Iteration\n",
    "def vi_step(Vprev):\n",
    "    Vnext, argmax = {}, {}\n",
    "    \n",
    "    # For each state, compute the updated value\n",
    "    for s in states:\n",
    "        best, besta, bestsp = -1e9, None, None  # Initialize with a very low value\n",
    "        \n",
    "        # Try each action from this state\n",
    "        for a in A:\n",
    "            sp = T[s][a]  # Next state after action a\n",
    "            val = R[s] + gamma * Vprev[sp]  # Bellman backup: reward + discounted value of next state\n",
    "            \n",
    "            # Keep the best action and value\n",
    "            if val > best:\n",
    "                best, besta, bestsp = val, a, sp\n",
    "        \n",
    "        # Store best value and the greedy action\n",
    "        Vnext[s] = best\n",
    "        argmax[s] = (besta, bestsp)  # Also keep track of which action was best\n",
    "    \n",
    "    return Vnext, argmax\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Run Value Iteration Steps\n",
    "# -------------------------\n",
    "\n",
    "# Iteration 0: initialize all state values to 0\n",
    "V0 = {s: 0.0 for s in states}\n",
    "print(\"Iteration 0:\", V0)\n",
    "\n",
    "# Iteration 1: update values once\n",
    "V1, A1 = vi_step(V0)\n",
    "print(\"Iteration 1:\", {s: round(V1[s],2) for s in states}, \" argmax:\", A1)\n",
    "\n",
    "# Iteration 2: update values again using results of iteration 1\n",
    "V2, A2 = vi_step(V1)\n",
    "print(\"Iteration 2:\", {s: round(V2[s],2) for s in states}, \" argmax:\", A2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6462f34f",
   "metadata": {},
   "source": [
    "### Interpretation of Results\n",
    "\n",
    "- **Iteration 0:** All state values start at `0.0`, with no preference for any action.  \n",
    "- **Iteration 1:** Rewards begin to propagate. State **s2** emerges as the most valuable, and other states start leaning toward it (e.g., `s3 → s1`, `s4 → s2`).  \n",
    "- **Iteration 2:** Values increase further. The policy refines itself, with most states directing transitions toward **s2**, which is becoming the optimal/goal state.  \n",
    "\n",
    "**Conclsuion:** Value iteration is successfully converging toward an optimal policy where all states prefer moving toward the highest-value state (**s2**).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
