{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00aacecb",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Programming - CSCN 8020\n",
    "\n",
    "## Assignment 1\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "### Done by ***Eris Leksi***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1199e7e9",
   "metadata": {},
   "source": [
    "Problem 1 [10]\n",
    "\n",
    "Pick-and-Place Robot: Consider using reinforcement learning to control the motion of a robot arm\n",
    "in a repetitive pick-and-place task. If we want to learn movements that are fast and smooth, the\n",
    "learning agent will have to control the motors directly and obtain feedback about the current positions\n",
    "and velocities of the mechanical linkages.\n",
    "Design the reinforcement learning problem as an MDP, define states, actions, rewards with reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e1c1fb",
   "metadata": {},
   "source": [
    "## 1. State Space (S)\n",
    "The state should capture robot dynamics, task progress, and safety:\n",
    "\n",
    "- Joint angles **qₜ** (pose)  \n",
    "- Joint velocities **q̇ₜ** (smoothness)  \n",
    "- Gripper state **gₜ ∈ {0,1}** (open/closed)  \n",
    "- Object pose **p_obj** and goal pose **p_goal**  \n",
    "- Holding flag **hₜ ∈ {0,1}** (object grasped or not)  \n",
    "- Collision flag **cₜ ∈ {0,1}** (safety)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87e392e",
   "metadata": {},
   "source": [
    "## 2. Action Space (A)\n",
    "The agent directly controls the robot through:\n",
    "\n",
    "- **Joint torques** or **velocity commands** (continuous)  \n",
    "- **Gripper command** (open/close)  \n",
    "\n",
    "This allows the agent to trade off speed and smoothness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f85575",
   "metadata": {},
   "source": [
    "## 3. Transition Dynamics (P)\n",
    "- Joint states update according to robot physics with noise.  \n",
    "- Gripper closing near object → holding flag set.  \n",
    "- Gripper opening at goal → successful placement.  \n",
    "- Collisions trigger safety flag and may end the episode.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80028420",
   "metadata": {},
   "source": [
    "## 4. Reward Function (R)\n",
    "Encourages **fast, smooth, safe** task completion:\n",
    "\n",
    "- **Task rewards**:  \n",
    "  - +1 for successful pick  \n",
    "  - +2 for successful place at goal  \n",
    "\n",
    "- **Shaping terms**:  \n",
    "  - Small step penalty → faster completion  \n",
    "  - Distance penalty → stay close to object/goal  \n",
    "  - Energy/torque penalty → discourage inefficiency  \n",
    "  - Jerk penalty → encourage smooth actions  \n",
    "  - Large penalty for collisions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d139dae3",
   "metadata": {},
   "source": [
    "## 5. Discount Factor (γ)\n",
    "- **γ ∈ [0.95, 0.995]**  \n",
    "- Balances valuing future rewards while favoring quick completion.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214f49b3",
   "metadata": {},
   "source": [
    "## 6. Terminal Conditions\n",
    "- **Success**: object placed at goal and released  \n",
    "- **Failure**: collision or time limit exceeded  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5568d8",
   "metadata": {},
   "source": [
    "## 9. Pseudocode (SAC Training Loop)\n",
    "\n",
    "```python\n",
    "initialize SAC agent \n",
    "initialize replay buffer B\n",
    "\n",
    "for episode in range(N):\n",
    "    s = env.reset()\n",
    "    for t in range(T):\n",
    "        a = πθ(s) + exploration_noise\n",
    "        s', r, done = env.step(a)\n",
    "        B.add(s, a, r, s', done)\n",
    "\n",
    "        if len(B) > warmup:\n",
    "            for _ in range(updates_per_step):\n",
    "                batch = B.sample()\n",
    "                update critics and policy\n",
    "                update target networks\n",
    "\n",
    "        s = s'\n",
    "        if done: break\n",
    "\n",
    "    evaluate policy periodically without noise"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
